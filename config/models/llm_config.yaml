# LLM Configuration
# Change the 'default' value to switch between providers
# Add your API keys in the .env file

default: "openrouter"  # Options: openai, claude, gemini, ollama, groq, openrouter

providers:
  openai:
    model: "gpt-4o"
    api_key_env: "OPENAI_API_KEY"
    base_url: null
    temperature: 0.7
    max_tokens: 4096
    timeout: 60

  claude:
    model: "claude-3-5-sonnet-20241022"
    api_key_env: "ANTHROPIC_API_KEY"
    base_url: null
    temperature: 0.7
    max_tokens: 4096
    timeout: 60

  gemini:
    model: "gemini-2.0-flash"
    api_key_env: "GOOGLE_API_KEY"
    temperature: 0.7
    max_tokens: 8192
    timeout: 60

  ollama:
    model: "llama3.1"
    base_url: "http://localhost:11434"
    temperature: 0.7
    max_tokens: 4096
    timeout: 120

  groq:
    model: "llama-3.3-70b-versatile"
    api_key_env: "GROQ_API_KEY"
    base_url: "https://api.groq.com/openai/v1"
    temperature: 0.7
    max_tokens: 4096
    timeout: 60

  openrouter:
    model: "zhipu/glm-4-plus" # Or moonshotai/moonshot-v1-8k for Kimi
    api_key_env: "OPENROUTER_API_KEY"
    base_url: "https://openrouter.ai/api/v1"
    temperature: 0.7
    max_tokens: 8192
    timeout: 60

# Model selection for specific tasks (override default)
task_models:
  resume_analysis: null
  interview_questions: null
  response_evaluation: null
  report_generation: null
